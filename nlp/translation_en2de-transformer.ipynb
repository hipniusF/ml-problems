{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c9048c0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchtext import data, datasets\n",
    "import spacy\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "dev = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d34ac724",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating src vocab...\n",
      "Generating trg vocab...\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "from utils import WMT14En2DeDatasetTokenizer, get_trainer_model\n",
    "dataset = WMT14En2DeDatasetTokenizer(dev=dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "673286f0",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "dl = DataLoader(\n",
    "    dataset.train,\n",
    "    shuffle=True,\n",
    "    batch_size=1,\n",
    "    collate_fn=dataset.collate_fn)\n",
    "example =  next(iter(dl))\n",
    "{a: example[a].shape for a in example}\n",
    "a = []\n",
    "for i, batch in enumerate(dl):\n",
    "    if i > 10000:\n",
    "        break\n",
    "    a.append(batch['ntokens'].cpu())\n",
    "a = np.array(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "138f2ae3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.7977470317473374"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "25000 / 350 / a.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d91ee33c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "26807.28427157284"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.mean() * 350 * 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c0c0caf",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f232262d",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "error:  No checkpoints in ./chkpnts\n"
     ]
    }
   ],
   "source": [
    "from transformers.model import EncoderDecoder, Trainer\n",
    "\n",
    "trainer, model = get_trainer_model(dataset, EncoderDecoder, Trainer, dev=dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "90e220dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'69.978857 * 1e6 parameters'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f'{sum(p.numel() for p in model.parameters()) / 1e6} * 1e6 parameters'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "595951e6",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bff75f3394ca4f3f95f5bcd319006812",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [5]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_loop\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1_000_000\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m350\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msave\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnotify\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/fun/ml-problems/nlp/transformers/model.py:304\u001b[0m, in \u001b[0;36mTrainer.train_loop\u001b[0;34m(self, steps, batch_size, accum_steps, save, notify)\u001b[0m\n\u001b[1;32m    301\u001b[0m trg_y \u001b[38;5;241m=\u001b[39m batch[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrg_y\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m    302\u001b[0m ntokens \u001b[38;5;241m=\u001b[39m batch[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mntokens\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m--> 304\u001b[0m y_hat \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43msrc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtgt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msrc_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtgt_mask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    305\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloss_fn(\n\u001b[1;32m    306\u001b[0m     y_hat\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mtgt_vocab), trg_y\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)) \u001b[38;5;66;03m#/ ntokens\u001b[39;00m\n\u001b[1;32m    307\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.9/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/fun/ml-problems/nlp/transformers/model.py:192\u001b[0m, in \u001b[0;36mEncoderDecoder.forward\u001b[0;34m(self, src, tgt, src_mask, tgt_mask)\u001b[0m\n\u001b[1;32m    191\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, src, tgt, src_mask, tgt_mask):\n\u001b[0;32m--> 192\u001b[0m     memory \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencode\u001b[49m\u001b[43m(\u001b[49m\u001b[43msrc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msrc_mask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    193\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdecode(tgt, tgt_mask, memory, src_mask)\n\u001b[1;32m    194\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgenerator(x)\n",
      "File \u001b[0;32m~/fun/ml-problems/nlp/transformers/model.py:198\u001b[0m, in \u001b[0;36mEncoderDecoder.encode\u001b[0;34m(self, src, mask)\u001b[0m\n\u001b[1;32m    196\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mencode\u001b[39m(\u001b[38;5;28mself\u001b[39m, src, mask):\n\u001b[1;32m    197\u001b[0m     src \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msrc_embed(src)\n\u001b[0;32m--> 198\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43msrc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmask\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.9/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/fun/ml-problems/nlp/transformers/model.py:143\u001b[0m, in \u001b[0;36mEncoder.forward\u001b[0;34m(self, x, mask)\u001b[0m\n\u001b[1;32m    141\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x, mask\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m    142\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m att, ff \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39matts, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mffs):\n\u001b[0;32m--> 143\u001b[0m         x \u001b[38;5;241m=\u001b[39m \u001b[43matt\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    144\u001b[0m         x \u001b[38;5;241m=\u001b[39m ff(x)\n\u001b[1;32m    145\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.9/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/fun/ml-problems/nlp/transformers/model.py:113\u001b[0m, in \u001b[0;36mEncoderResidualLayer.forward\u001b[0;34m(self, x, mask)\u001b[0m\n\u001b[1;32m    111\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x, mask\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m    112\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m mask \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 113\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm(x \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msublayer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmask\u001b[49m\u001b[43m)\u001b[49m))\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    115\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm(x \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msublayer(x)))\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.9/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/fun/ml-problems/nlp/transformers/model.py:55\u001b[0m, in \u001b[0;36mMultiHeadAttention.forward\u001b[0;34m(self, x, mask)\u001b[0m\n\u001b[1;32m     53\u001b[0m batch_size \u001b[38;5;241m=\u001b[39m q\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m     54\u001b[0m q \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlv(q)\u001b[38;5;241m.\u001b[39mview(batch_size, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mh, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdim_k)\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m---> 55\u001b[0m k \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mk\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mview(batch_size, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mh, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdim_k)\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m     56\u001b[0m v \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlv(v)\u001b[38;5;241m.\u001b[39mview(batch_size, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mh, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdim_k)\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m     57\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mattention(q, k, v, mask)\u001b[38;5;241m.\u001b[39mcontiguous()\u001b[38;5;241m.\u001b[39mview(batch_size, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mh\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdim_k)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.9/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.9/site-packages/torch/nn/modules/linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 114\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer.train_loop(1_000_000, batch_size=350, save=True, notify=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32d32f7d",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "test_dl = DataLoader(\n",
    "    dataset.train, shuffle=True, batch_size=1, collate_fn=dataset.collate_fn)\n",
    "example =  next(iter(test_dl))\n",
    "\n",
    "src = example['src']\n",
    "src_mask = example['src_mask']\n",
    "\n",
    "model.eval()\n",
    "trg = model.translate(\n",
    "    src, src_mask, dataset, dev=dev)\n",
    "dataset.itos(src[0], field='src'), dataset.itos(trg[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2207e45d",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(trainer.losses)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "e7cb1b9ae4d417fedf7f40a8eec98f7cfbd359e096bd857395a915f4609834ce"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

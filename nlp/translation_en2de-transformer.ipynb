{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c9048c0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torchtext.datasets import Multi30k\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "from tqdm.notebook import tqdm\n",
    "import spacy\n",
    "from spacy.tokenizer import Tokenizer\n",
    "\n",
    "dev = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1f1cadcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from urllib.request import urlopen\n",
    "import linecache\n",
    "from itertools import count\n",
    "import pickle\n",
    "\n",
    "class En2DeDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, folder_path, transform=None, download=False, train=True):\n",
    "        self.path = Path(folder_path)\n",
    "        self.train = train\n",
    "        self.transform = transform\n",
    "        self.train_en_url = 'https://nlp.stanford.edu/projects/nmt/data/wmt14.en-de/train.en'\n",
    "        self.train_de_url = 'https://nlp.stanford.edu/projects/nmt/data/wmt14.en-de/train.de'\n",
    "\n",
    "        self.test_en_url = 'https://nlp.stanford.edu/projects/nmt/data/wmt14.en-de/newstest2015.en'\n",
    "        self.test_de_url = 'https://nlp.stanford.edu/projects/nmt/data/wmt14.en-de/newstest2015.de'\n",
    "\n",
    "        self.length = 4_468_841 if train else 2_170\n",
    "        if train:\n",
    "            try:\n",
    "                with (self.path / 'vocabs').open('rb') as f:\n",
    "                    self.vocabs = pickle.load(f)  \n",
    "            except FileNotFoundError:\n",
    "                if download:\n",
    "                    self._download()\n",
    "                else:\n",
    "                    raise FileNotFoundError('Set download to True to download the dataset')\n",
    "        files = ('train.en', 'train.de') if self.train else ('test.en', 'test.de')\n",
    "        line_path = self.path / files[0]\n",
    "        label_path = self.path / files[1]\n",
    "        if not line_path.exists() or not label_path.exists():\n",
    "            if download:\n",
    "                self._download()\n",
    "            else:\n",
    "                raise FileNotFoundError('Set download to True to download the dataset')     \n",
    "    \n",
    "    def _download(self):\n",
    "        self.path.mkdir(parents=True, exist_ok=True)\n",
    "        if self.train:\n",
    "            files = (('en', 'train.en', self.train_en_url),\n",
    "                     ('de', 'train.de', self.train_de_url))\n",
    "        else:\n",
    "            files = (('en', 'test.en', self.test_en_url),\n",
    "                     ('de', 'test.de', self.test_de_url))\n",
    "\n",
    "        self.vocabs = {'en': set(), 'de': set()}\n",
    "        for lang, file, url in files:\n",
    "            with urlopen(url) as webfile:\n",
    "                localpath = self.path / file\n",
    "                if localpath.exists():\n",
    "                    localpath.unlink()\n",
    "                with localpath.open(\"wb+\") as localfile:\n",
    "                    for i in tqdm(range(self.length)):\n",
    "                        line = webfile.readline()\n",
    "                        if self.train:\n",
    "                            self.vocabs[lang].update(line.decode(\"utf-8\").casefold().split(' '))\n",
    "                        localfile.write(line)\n",
    "                    assert(not line)\n",
    "        vocab_path = self.path / 'vocabs'\n",
    "        with vocab_path.open('wb') as f:\n",
    "            pickle.dump(self.vocabs, f)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.length\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        files = ('train.en', 'train.de') if self.train else ('test.en', 'test.de')\n",
    "        line_path = self.path / files[0]\n",
    "        label_path = self.path / files[1]\n",
    "        if not line_path.exists() or not label_path.exists():\n",
    "            raise FileNotFoundError('Set download to True to download the dataset')\n",
    "        \n",
    "        line = linecache.getline(str(line_path.absolute()), idx)\n",
    "        label = linecache.getline(str(label_path.absolute()), idx)\n",
    "\n",
    "        if self.transform:\n",
    "            line = self.transform(line)\n",
    "            label = self.transform(label)\n",
    "        return line, label\n",
    "\n",
    "\n",
    "train_dataset = En2DeDataset('./downloads', download=True, train=True)\n",
    "test_dataset = En2DeDataset('./downloads', download=True, train=False)\n",
    "\n",
    "train_dtld = torch.utils.data.DataLoader(train_dataset, shuffle=True, batch_size=16)\n",
    "test_dtld = torch.utils.data.DataLoader(test_dataset, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ca91b0eb",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "__init__() missing 2 required positional arguments: 'exts' and 'fields'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [3]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m dtset \u001b[38;5;241m=\u001b[39m \u001b[43mMulti30k\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m./downloads/\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msplit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtrain\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlanguage_pair\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43men\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mde\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mTypeError\u001b[0m: __init__() missing 2 required positional arguments: 'exts' and 'fields'"
     ]
    }
   ],
   "source": [
    "dtset = IWSLT(\n",
    "    './downloads/', split='train', language_pair=('en','de'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "95f286fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1530303"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_dataset.vocabs['de'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d6636139",
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.lang.en import English\n",
    "lang = English()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5f6f6410",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "this is a sentence"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Tokenizer(lang.vocab)(\"this is a sentence\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "e7cb1b9ae4d417fedf7f40a8eec98f7cfbd359e096bd857395a915f4609834ce"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
